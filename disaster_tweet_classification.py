# -*- coding: utf-8 -*-
"""Disaster Tweet Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kHt9mKDQ3D_jA8h0vJZhxmrwRj0ZhrAf

1. Setting Up the Environment
âœ… Tools You'll Need:
Python 3.8 or above â€” The primary language for this project.
Jupyter Notebook or Google Colab â€” Ideal for data science projects because of their interactive nature.
Libraries to Install:
pandas â†’ For data handling.
numpy â†’ For numerical operations.
nltk â†’ For text preprocessing.
gensim â†’ For Word2Vec embeddings.
transformers â†’ To use Hugging Face models.
sklearn â†’ For evaluation metrics.
ðŸ’» Installing Libraries:
If you're using Google Colab, run this in the first cell:

!pip install pandas numpy nltk gensim transformers scikit-learn
"""

pip install pandas

!pip install pandas numpy nltk gensim transformers scikit-learn matplotlib wordcloud

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from transformers import pipeline
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

""" 2. Getting the Dataset
Go to Kaggle: NLP Getting Started Dataset
Sign in and Join the Competition (if needed).
Download the train.csv file.
Upload it to your Colab/Jupyter Notebook.
"""

from google.colab import files
uploaded = files.upload()

"""3. Understanding the Dataset
The CSV file has the following columns:

id â€” Unique identifier for each tweet.
text â€” The tweet itself.
target â€” 1 if it's a disaster tweet, 0 otherwise.
To view the data:
"""

import pandas as pd

# Load the CSV file
data = pd.read_csv('train.csv')

# View the first 5 rows
data.head()

"""4. Data Preprocessing (Cleaning the Tweets)
Tweets are messy! They contain URLs, emojis, mentions, etc. We need to clean them.

âœ… Steps to Clean Data:
Remove URLs, mentions, and hashtags.
Convert text to lowercase.
Remove punctuation and stopwords.
Lemmatize words (reduce them to their root form).
Code Example:
"""

import nltk

# Force download punkt_tab
nltk.download('punkt')
nltk.download('punkt_tab')

# Verify if it's installed
nltk.data.find('tokenizers/punkt')

# Apply the cleaning function again
data['cleaned_text'] = data['text'].apply(clean_text)

# Check cleaned data
data[['text', 'cleaned_text']].head()

""" Step 6: Clean the Tweets
Now, let's clean the data using NLTK:
"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = re.sub(r'http\\S+', '', text)  # Remove URLs
    text = re.sub(r'@\\w+', '', text)     # Remove mentions
    text = re.sub(r'#', '', text)         # Remove hashtags
    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove punctuation
    text = text.lower()  # Lowercase
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Apply the cleaning function
data['cleaned_text'] = data['text'].apply(clean_text)

# Check cleaned data
data[['text', 'cleaned_text']].head()

""" Step 7: Word Embedding with Word2Vec
Letâ€™s create word embeddings:
"""

# View all words in the vocabulary
vocab = list(word2vec_model.wv.index_to_key)
print("Vocabulary Size:", len(vocab))
print("Sample words:", vocab[:20])  # Show first 20 words

def clean_text(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)     # Remove mentions
    text = re.sub(r'#', '', text)        # Remove hashtags
    text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove punctuation
    text = text.lower()  # Lowercase
    tokens = nltk.word_tokenize(text)  # Proper tokenization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)  # Join tokens with spaces

# Apply the updated cleaning function
data['cleaned_text'] = data['text'].apply(clean_text)

# Verify cleaned data
data[['text', 'cleaned_text']].head()

# Tokenize the cleaned text
tokenized_sentences = [nltk.word_tokenize(text) for text in data['cleaned_text']]

# Rebuild Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)

# Check the updated vocabulary
vocab = list(word2vec_model.wv.index_to_key)
print("Updated Vocabulary Size:", len(vocab))
print("Sample words:", vocab[:20])

if 'fire' in word2vec_model.wv.key_to_index:
    print(word2vec_model.wv.most_similar('fire'))
else:
    print("'fire' is still not in the vocabulary. Try another common word from the dataset.")

# Find words similar to 'fire'
print("Words similar to 'fire':")
print(word2vec_model.wv.most_similar('fire'))

def clean_text_improved(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)     # Remove mentions
    text = re.sub(r'#', '', text)        # Remove hashtags
    text = re.sub(r'&amp;', 'and', text)  # Fix HTML entities
    text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove punctuation
    text = text.lower()  # Lowercase
    tokens = nltk.word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Apply improved cleaning
data['cleaned_text'] = data['text'].apply(clean_text_improved)

# Tokenize cleaned text again
tokenized_sentences = [nltk.word_tokenize(text) for text in data['cleaned_text']]

# Rebuild Word2Vec with new params
word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=200, window=10, min_count=1, workers=4)

# Check 'fire' similarities again
if 'fire' in word2vec_model.wv.key_to_index:
    print("Words similar to 'fire':", word2vec_model.wv.most_similar('fire'))
else:
    print("'fire' is still missing. Try another word.")

import gensim.downloader as api

# Load pre-trained GloVe embeddings
glove_vectors = api.load("glove-wiki-gigaword-100")

# Check similar words to 'fire'
print(glove_vectors.most_similar('fire'))

"""ðŸ“Œ Step 1: Install Hugging Face Transformers (if not already installed)"""

!pip install transformers

""" Step 2: Import the Zero-Shot Classification Pipeline"""

from transformers import pipeline

# Load Hugging Face zero-shot classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

"""ðŸ“Œ Step 3: Test Zero-Shot on a Single Tweet"""

# Example tweet
tweet = "A massive fire broke out downtown, and emergency services are on the scene."

# Candidate labels
labels = ["disaster", "non-disaster"]

# Apply zero-shot classification
result = classifier(tweet, labels)

# Display result
print("Label:", result['labels'][0])
print("Scores:", result['scores'])

""" Step 4: Apply Zero-Shot to the Entire Dataset"""

def zero_shot_classify(text):
    labels = ["disaster", "non-disaster"]
    result = classifier(text, labels)
    return result['labels'][0]  # Get the top label

# Apply classification to each tweet
data['predicted'] = data['text'].apply(zero_shot_classify)

# View some predictions
data[['text', 'predicted']].head()

# Sample 500 tweets for faster processing
sample_data = data.sample(n=500, random_state=42).copy()

# Apply Zero-Shot Classification on the sample
sample_data['predicted'] = sample_data['text'].apply(zero_shot_classify)

# Evaluate on the sample
from sklearn.metrics import accuracy_score
sample_data['predicted_binary'] = sample_data['predicted'].apply(lambda x: 1 if x == 'disaster' else 0)
accuracy = accuracy_score(sample_data['target'], sample_data['predicted_binary'])
print(f"Sample Accuracy: {accuracy * 100:.2f}%")

import torch
print("GPU available:", torch.cuda.is_available())

from tqdm import tqdm  # For progress bar

def batch_zero_shot_classify(texts, batch_size=8):
    labels = ["disaster", "non-disaster"]
    predictions = []
    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i + batch_size]
        results = classifier(batch.tolist(), labels)
        for res in results:
            predictions.append(res['labels'][0])
    return predictions

# Apply batched classification
data['predicted'] = batch_zero_shot_classify(data['text'].values)

"""ðŸ“Š Step 1: View Predictions"""

# View first 10 predictions
data[['text', 'predicted']].head(10)

"""âœ… Step 2: Evaluate Model Accuracy
Letâ€™s see how well the model performed by comparing its predictions against the actual labels (target column).
"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Convert predictions to binary (1 for disaster, 0 for non-disaster)
data['predicted_binary'] = data['predicted'].apply(lambda x: 1 if x == 'disaster' else 0)

# Calculate accuracy
accuracy = accuracy_score(data['target'], data['predicted_binary'])
print(f"Zero-Shot Classification Accuracy: {accuracy * 100:.2f}%")

# Detailed classification report
print("\nClassification Report:\n")
print(classification_report(data['target'], data['predicted_binary']))

# Confusion matrix for visualization
cm = confusion_matrix(data['target'], data['predicted_binary'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Disaster', 'Disaster'], yticklabels=['Non-Disaster', 'Disaster'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""ðŸ“ˆ Step 3: Visualize Disaster vs Non-Disaster Tweets
Letâ€™s plot a bar chart to see the distribution of predictions.
"""

# Count of disaster vs non-disaster predictions
data['predicted'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Predicted Tweet Categories')
plt.xlabel('Category')
plt.ylabel('Number of Tweets')
plt.show()